{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Roman Science Platform Data Discovery and Access in the Cloud \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Imports](#Imports)\n",
    "* [Introduction](#Introduction)\n",
    "* [Accessing MAST Data](#Accessing-MAST-Data)\n",
    "* [Streaming ASDF Files](#Streaming-ASDF-Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports\n",
    "Here we import the required packages for our data access examples including:\n",
    "- *astropy.io fits* for accessing FITS files\n",
    "- *astropy.mast Observations* for accessing, searching, and selecting data from other missions\n",
    "- *s3fs* for streaming in data directly from the cloud\n",
    "- *roman_datamodels* for opening Roman ASDF files (This is not a focus of this notebook, but solely to show that we can actually access the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import s3fs\n",
    "import roman_datamodels as rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "This notebook is designed to provide examples for how to access data from the science platform. In particular, it demonstrates how to stream data from the cloud directly into memory, circumventing the need to download the data locally and use excess storage. This method of accessing the data on the cloud is *HIGHLY* recommended, however we understand that some use-cases will require the data to be downloaded locally so we show an example of how to do that at the end of the notebook.\n",
    "\n",
    "Here-in we examine how to download data from two types of sources:\n",
    "- the STScI MAST server which hosts data for in-flight telescopes including Hubble, TESS, and JWST\n",
    "- simulated Roman Space Telescope data hosted in storage containers on the AWS cloud\n",
    "\n",
    "\n",
    "### Defining terms\n",
    "- Cloud computing: the practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer\n",
    "- AWS: Amazon Web Services (AWS) is the cloud computing landscape that is provided by Amazon\n",
    "- URI: a Universal Resource Identifier (URI) is a sequence of characters that identify a name or a unique resource on the Internet. Website URLs are a subclass of URIs.\n",
    "- AWS S3: Amazon Simple Storage Service (S3) is a very scalable and inexpensive object storage service on the AWS cloud platform. The storage containers are referred to as \"buckets\" so we will often refer to these storage devices as \"S3 buckets\" or \"S3 servers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing MAST Data\n",
    "In this section, we will go through the steps to retreive archived MAST data from the cloud including how to query the archive, stream the files directly from the cloud, as well as download them locally.\n",
    "\n",
    "### Enabling Cloud Access\n",
    "The most important step for accessing data from the cloud is to enable *astroquery* to retreive URIs and other relevant cloud information. Even if we are working locally and plan to download the data files (not recommended for Roman data), we need to use this command to copy the file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Observations.enable_cloud_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying MAST\n",
    "Now we are ready to begin our query. This example is rather simple, but is quick and easy to reproduce. We will be querying Hubble ACS/WFC data in this example. In our query, we specify that we want to look at HST data using the F606W filter and ACS/WFC. We also specify the proposal id to easily get the data of interest. Once we get the desired observations, we gather the list of products that go into the obervations. We then filter the products to gather all the drizzled data products (as they have higher resolution and will look better with simple plotting) which leaves us with four filtered products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = Observations.query_criteria(obs_collection='HST',\n",
    "                                  filters='F606W',\n",
    "                                  instrument_name='ACS/WFC',\n",
    "                                  proposal_id=['12062'],\n",
    "                                  dataRights='PUBLIC')\n",
    "products = Observations.get_product_list(obs)\n",
    "filtered = Observations.filter_products(products,\n",
    "                                        productSubGroupDescription='DRZ')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our desired products, we can gather the URIs for each of the files which indicate the files' locations in the MAST AWS S3 servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = Observations.get_cloud_uris(filtered)\n",
    "uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `get_cloud_uris` checks for duplicates in the provided products to reduce the data access volume. It is also important to not that `get_cloud_uris` will always return a list. Thus we will need to gather the indivual URI string to access the files. Let's choose the first URI for the remainder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = uris[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming files directly into memory\n",
    "Here, we will use `s3fs` to directly access the data stored in the AWS S3 servers. Note that we must set `anon=True` to acces the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can see that the URI points to a FITS file, we can use `astropy` to access the information in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in AWS: 'F' is the S3 file\n",
    "with fs.open(uri, 'rb') as f:\n",
    "    # Now actually read in the FITS file \n",
    "    with fits.open(f, 'readonly') as HDUlist:\n",
    "        HDUlist.info()\n",
    "        sci = HDUlist[1].data\n",
    "type(sci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming ASDF Files\n",
    "\n",
    "Though Roman data will eventually be available through MAST, it will not be in this testing phase, so some simulated data have been placed in a separate S3 bucket. These files can be streamed in exactly the same way as the Hubble FITS file above. Additionally we can also peruse the available files similarly to a Unix terminal. A full list of commands can be found in the `s3fs` documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf_dir_uri = 's3://roman-sci-test-data-prod-summer-beta-test/'\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "fs.ls(asdf_dir_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `fs.ls()` command allows the contents of the URI to be listed. In the above example, the `roman-sci-test-data-prod-summer-beta-test` s3 bucket should contain two directories:\n",
    "- `ROMANISIM` contains the simulated WFI-imaging mode Roman Space Telescope data used in this suite of notebooks\n",
    "- `STIPS` contains data for the Space Telescope Image Product Simulator (STIPS) notebook (Notebook link: [stips.ipynb](../stips/stips.ipynb))\n",
    "\n",
    "Diving into the `ROMANISIM` directory, we find three folders:\n",
    "- `CATALOGS_SCRIPTS` containing stellar and galactic catalogs used to create the simulated data stored in the other directories\n",
    "- `DENSE_REGION` containing calibrated and uncalibrated simulated data of dense stellar fields at different filters for all WFI detectors. The data are separarted into two directories, each with a different pointings. Filenames in these directories use the prefixes `r0000101001001001001*` and `r0000101001001001002*` which correspond to the use of the F158 and F129 optical elements respectively.\n",
    "- `GALAXIES` containing one calibrated, simulated image containing galaxies using the F158 optical element.\n",
    "\n",
    "Below we use `roman_datamodels` to read in a dense region asdf file as an example. Please see the [working_with_asdf.ipynb](../working_with_asdf/working_with_asdf.ipynb) notebook for more information about accessing data within asdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf_file_uri = asdf_dir_uri + 'ROMANISIM/DENSE_REGION/R0.5_DP0.5_PA0/r0000101001001001001_01101_0001_WFI01_cal.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri, 'rb') as f:\n",
    "    dm = rdm.open(f)\n",
    "    \n",
    "print(type(dm))\n",
    "print(dm.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Files Locally (not recommended)\n",
    "\n",
    "Though it is **not recommended**, there may be instances where data files must be downloaded locally for certain workflows. To do that, we can use the URIs and the `S3FileSystem.get` function (documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem.get))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out as this use case is not recommended and should only be needed in rare circumstances\n",
    "# from pathlib import Path\n",
    "# local_file_path = Path('data/')\n",
    "# local_file_path.mkdir(parents=True, exist_ok=True)\n",
    "# fs.get(uri, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Resources\n",
    "Some additional information that may be helpful can be found at the following links:\n",
    "\n",
    "- [`s3fs` Documentation](https://s3fs.readthedocs.io/en/latest/api.html#)\n",
    "- [Working with ASDF Notebook](../working_with_asdf/working_with_asdf.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this notebook\n",
    "The data streaming information from this notebook largely builds off of the TIKE data-acces notebook by Thomas Dutkiewicz.\n",
    "\n",
    "**Author:** Will C. Schultz  \n",
    "**Updated On:** 2024-05-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
